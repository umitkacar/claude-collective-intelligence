# Logstash Output Configuration
# Routes processed logs to multiple destinations

output {
  # ============ PRIMARY OUTPUT: ELASTICSEARCH ============

  elasticsearch {
    hosts => ["${ELASTICSEARCH_HOSTS:elasticsearch-main:9200}"]
    index => "%{[@metadata][index]}"
    document_type => "_doc"
    codec => "json"
    bulk_actions => 1000
    bulk_size => 5242880  # 5MB
    flush_interval => 5
    request_timeout => 60
    connect_timeout => 10
    socket_timeout => 60
    retry_on_conflict => 3
    action => "index"

    # User authentication (if enabled)
    user => "${ELASTICSEARCH_USER:elastic}"
    password => "${ELASTICSEARCH_PASSWORD:changeme}"
    ssl => false
    verify_mode => "none"

    # Performance tuning
    template => "/usr/share/logstash/templates/logs-template.json"
    template_name => "logs-template"
    template_overwrite => false
    manage_template => false
    ilm_enabled => true
    ilm_pattern => "logs-%{[service]}-{now/d}"
    ilm_rollover_alias => "logs-write"
    ilm_policy => "logs-policy"
  }

  # ============ SECONDARY OUTPUT: BACKUP INDEX ============

  # Optional: Send to secondary Elasticsearch for disaster recovery
  # elasticsearch {
  #   hosts => ["elasticsearch-backup:9200"]
  #   index => "logs-backup-%{+YYYY.MM.dd}"
  #   document_type => "_doc"
  #   user => "elastic"
  #   password => "${ELASTICSEARCH_PASSWORD}"
  #   ssl => false
  # }

  # ============ ERROR ALERTING ============

  # Send critical errors to webhook for immediate notification
  if [level] == "error" and [error_category] =~ /security|critical/ {
    http {
      url => "${ALERT_WEBHOOK_URL:http://alertmanager:9093/api/v1/alerts}"
      http_method => "post"
      format => "json"
      mapping => {
        "status" => "firing"
        "labels" => {
          "service" => "%{[service]}"
          "severity" => "critical"
          "error_category" => "%{[error_category]}"
        }
        "annotations" => {
          "summary" => "%{[message]}"
          "description" => "Critical error in %{[service]}: %{[message]}"
          "trace_id" => "%{[trace_id]}"
        }
      }
    }
  }

  # ============ SLACK NOTIFICATIONS (Optional) ============

  # Send error notifications to Slack
  if [level] == "error" {
    http {
      url => "${SLACK_WEBHOOK_URL}"
      http_method => "post"
      format => "message"
      message => '[%{[level]}] %{[service]} - %{[message]} (trace_id: %{[trace_id]})'
    }
  }

  # ============ STDOUT (Debug Only) ============

  # Output to console for debugging (remove in production)
  if [@metadata][debug] {
    stdout {
      codec => rubydebug
    }
  }

  # ============ FILE OUTPUT (Optional) ============

  # Write logs to file for local archival
  file {
    path => "/usr/share/logstash/logs/%{[service]}-%{[environment]}-%{+YYYY.MM.dd}.log"
    codec => json_lines
    create_if_deleted => true
    dir_perm => "0750"
    file_perm => "0640"
    flush_interval => 10
    rotation_strategy => "size"
    max_file_size => 1073741824  # 1GB
    max_backups => 10
  }

  # ============ METRICS EXPORT ============

  # Send metrics to external monitoring system
  if [duration_ms] {
    statsd {
      host => "prometheus"
      port => 8125
      namespace => "logstash"
      increment => ["logs.processed"]
      timing => {
        "logs.processing_time" => "%{[duration_ms]}"
      }
      sample_rate => 0.1
      tags => ["service:%{[service]}", "level:%{[level]}", "environment:%{[environment]}"]
    }
  }

  # ============ METRICS AGGREGATION ============

  # Send to Elasticsearch for metrics
  if [http_status] {
    elasticsearch {
      hosts => ["${ELASTICSEARCH_HOSTS:elasticsearch-main:9200}"]
      index => "metrics-http-%{+YYYY.MM.dd}"
      document_type => "_doc"
      user => "${ELASTICSEARCH_USER:elastic}"
      password => "${ELASTICSEARCH_PASSWORD:changeme}"
      ssl => false
    }
  }

  # ============ ARCHIVE STORAGE (Optional) ============

  # Archive to S3 for long-term storage
  # s3 {
  #   access_key_id => "${AWS_ACCESS_KEY_ID}"
  #   secret_access_key => "${AWS_SECRET_ACCESS_KEY}"
  #   region => "${AWS_REGION:us-east-1}"
  #   bucket => "logstash-archives"
  #   size_file => 2147483648  # 2GB
  #   time_file => 60
  #   codec => "json"
  #   prefix => "logs/%{[service]}/%{+YYYY/MM/dd}"
  # }

  # ============ DEADLETTER QUEUE ============

  # Send logs with processing errors to DLQ
  if "_grokparsefailure" in [tags] or "_mutate_error" in [tags] {
    file {
      path => "/usr/share/logstash/dlq/dlq-%{+YYYY.MM.dd.HH}.log"
      codec => json_lines
      create_if_deleted => true
      dir_perm => "0750"
      file_perm => "0640"
    }
  }
}
